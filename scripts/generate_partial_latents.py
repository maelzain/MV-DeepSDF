#!/usr/bin/env python3

import multiprocessing as mp
mp.set_start_method('spawn', force=True)

import sys, os
# Set CUDA environment before importing torch to avoid initialization issues
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Ensure GPU 0 is visible
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # Consistent GPU ordering

# allow imports from project root
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

"""
Generate partial latent codes for each sweep in the multi-sweep point clouds.

This script performs the intermediate step for MV-DeepSDF Stage 2. It takes the
multi-sweep point clouds generated by `generate_multi_sweep_data.py` and, for
each sweep, infers an optimal latent code using a pre-trained DeepSDF decoder
from Stage 1.

The resulting partial latent codes (shape (6, 256)) are saved back into the
same .npz files, making them ready for `train_mvdeepsdf_stage2.py`.
"""

import argparse, glob, logging
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import json
import numpy as np
import torch
from tqdm import tqdm

from networks.deep_sdf_decoder import Decoder

logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s %(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


def infer_latent_code(decoder, points, latent_size=256, num_iter=800, lr=1e-4, reg_weight=1e-3, device='cpu'):
    """
    Infers an optimal latent code for a given single point cloud.
    This function implements the optimization described in Eq. (1) of the MV-DeepSDF paper.
    
    PAPER COMPLIANCE: Ensures latent codes are normalized to [-1,1] range as required by:
    "DeepSDF [31] normalizes the latent codes into the range [−1, 1]"
    """
    # Ensure points are on the correct device
    points = torch.from_numpy(points).float().to(device)
    num_pts = points.shape[0]

    # Initialize latent code with small random values in [-1,1] range (paper compliance)
    z = torch.randn(1, latent_size, device=device) * 0.1  # Small initialization
    z.requires_grad_(True)
    optimizer = torch.optim.Adam([z], lr=lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)
    loss_l1 = torch.nn.L1Loss()

    # The ground truth SDF for on-surface points is 0
    gt_sdf = torch.zeros(num_pts, 1).to(device)

    prev_loss = float('inf')
    patience = 50
    stale_count = 0

    for iteration in range(num_iter):
        optimizer.zero_grad()

        # Expand latent code to match number of points
        z_expanded = z.expand(num_pts, -1)

        # Predict SDF values
        combined_input = torch.cat([z_expanded, points], dim=1)
        pred_sdf = decoder(combined_input)

        # Compute loss components
        # L1 loss to fit the surface (SDF=0)
        fit_loss = loss_l1(pred_sdf, gt_sdf)
        
        # L2 regularization as per DeepSDF paper (Eq. 1)
        reg_loss = reg_weight * torch.mean(z.pow(2))
        
        # PAPER COMPLIANCE: Enforce [-1,1] range constraint using tanh-like penalty
        # This ensures latent codes stay within the expected range
        range_penalty = 0.0
        z_abs = torch.abs(z)
        out_of_range = torch.clamp(z_abs - 1.0, min=0.0)
        if torch.sum(out_of_range) > 0:
            range_penalty = 0.1 * torch.mean(out_of_range.pow(2))
            
        loss = fit_loss + reg_loss + range_penalty

        loss.backward()
        optimizer.step()
        
        # PAPER COMPLIANCE: Clamp latent codes to [-1,1] range after each update
        with torch.no_grad():
            z.clamp_(-1.0, 1.0)
        
        scheduler.step()

        # Early stopping based on convergence
        if abs(prev_loss - loss.item()) < 1e-6:
            stale_count += 1
            if stale_count >= patience:
                break
        else:
            stale_count = 0
        prev_loss = loss.item()

    # Final clamp to ensure [-1,1] range (paper compliance)
    with torch.no_grad():
        z.clamp_(-1.0, 1.0)

    return z.detach().cpu().numpy().squeeze()


def process_instance(args):
    """Load a .npz, infer 6 partial latents, and save them back to the file."""
    npz_path, decoder_state_dict, stage1_config, device_str = args
    
    # Handle CUDA context initialization for multiprocessing
    if device_str.startswith('cuda'):
        try:
            # Set CUDA environment variables for this worker process
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'
            os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
            
            # Force CUDA initialization
            torch.cuda.init()
            torch.cuda.empty_cache()  # Clear cache
            
            # Verify CUDA is actually working
            if torch.cuda.is_available() and torch.cuda.device_count() > 0:
                # Test a simple CUDA operation
                test_tensor = torch.ones(1, device='cuda')
                device_str = 'cuda'
                logger.info(f"CUDA successfully initialized in worker for {os.path.basename(npz_path)}")
            else:
                raise RuntimeError("CUDA not available after initialization")
                
        except Exception as e:
            device_str = 'cpu'
            logger.warning(f"CUDA initialization failed in worker: {e}, using CPU for {os.path.basename(npz_path)}")
    
    device = torch.device(device_str)

    # Each worker needs to instantiate its own model
    # Strip 'module.' prefix from state dict keys if present
    new_state_dict = {k.replace('module.', ''): v for k, v in decoder_state_dict.items()}
    # Build the decoder with the correct architecture from the config file
    decoder = Decoder(
        latent_size=stage1_config['latent_dim'],
        dims=stage1_config['decoder_dims'],
        dropout=stage1_config.get('dropout', None),
        dropout_prob=stage1_config.get('dropout_prob', 0.0),
        norm_layers=stage1_config.get('norm_layers', ()),
        latent_in=stage1_config.get('latent_in', ()),
        weight_norm=stage1_config.get('weight_norm', False),
        xyz_in_all=stage1_config.get('xyz_in_all', None),
        use_tanh=stage1_config.get('use_tanh', False),
        latent_dropout=stage1_config.get('latent_dropout', False)
    ).to(device)
    decoder.load_state_dict(new_state_dict)
    decoder.eval()
    
    # Clear any cached memory if using CUDA
    if device.type == 'cuda':
        torch.cuda.empty_cache()

    try:
        data = np.load(npz_path)
        
        # Robustly load point cloud data, matching the dataset loader
        if 'point_clouds' in data:
            key = 'point_clouds'
        elif 'point_cloud' in data:
            key = 'point_cloud'
        elif 'sweeps' in data:
            key = 'sweeps'  # Handle legacy format from earlier data generation
        else:
            available_keys = list(data.keys())
            logger.error(f"Could not find point cloud data in {os.path.basename(npz_path)}. Available keys: {available_keys}. Expected one of: ['point_clouds', 'point_cloud', 'sweeps']")
            return False
        point_clouds = data[key] # Shape (6, 256, 3)

        # Comment out this block to force regeneration of latent codes
        # if 'partial_latents' in data:
        #     return True

        partial_latents = []
        for i in range(point_clouds.shape[0]):
            pc_view = point_clouds[i]
            latent_vec = infer_latent_code(decoder, pc_view, device=device)
            partial_latents.append(latent_vec)

        partial_latents_np = np.stack(partial_latents, axis=0) # Shape (6, 256)

        # Save back to the same file, using the more general key 'point_clouds'
        np.savez(npz_path, point_clouds=point_clouds, partial_latents=partial_latents_np)
        
        # Clean up GPU memory after processing
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        
        return True

    except Exception as e:
        logger.error(f"Failed to process {os.path.basename(npz_path)}: {e}")
        
        # Clean up GPU memory on error
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        
        return False


def main():
    p = argparse.ArgumentParser(description="Generate partial latent codes for multi-sweep data")
    p.add_argument("--data_dir", required=True, help="Directory with .npz files from generate_multi_sweep_data.py")
    p.add_argument("--decoder", required=True, help="Path to pre-trained DeepSDF decoder checkpoint (.pth)")
    p.add_argument("--config", required=True, help="Path to the main mvdeepsdf_config.json file")
    p.add_argument("--jobs", type=int, default=4, help="Parallel worker count")
    p.add_argument("--smoke", action="store_true", help="Smoke-run on 10 instances")
    p.add_argument("--force_cpu", action="store_true", help="Force CPU usage even if CUDA is available")
    p.add_argument("--use_threading", action="store_true", help="Use threading instead of multiprocessing (better for CUDA)")
    args = p.parse_args()

    # Handle device selection with better CUDA management
    if args.force_cpu:
        device_str = 'cpu'
        logger.info("Forcing CPU usage as requested")
    elif torch.cuda.is_available():
        device_str = 'cuda'
        logger.info(f"CUDA available with {torch.cuda.device_count()} GPU(s)")
        # Automatically use threading for CUDA to avoid multiprocessing issues
        if device_str == 'cuda' and not args.use_threading:
            args.use_threading = True
            logger.info("Automatically using threading mode for CUDA (better than multiprocessing)")
        # Reduce worker count if using CUDA to prevent GPU memory issues
        if args.jobs > 2 and not args.use_threading:
            args.jobs = min(2, torch.cuda.device_count())
            logger.info(f"Reducing worker count to {args.jobs} for CUDA multiprocessing")
    else:
        device_str = 'cpu'
        logger.info("CUDA not available, using CPU")
    
    execution_mode = "threading" if args.use_threading else "multiprocessing"
    logger.info(f"Using device: {device_str} with {args.jobs} workers ({execution_mode})")

    # Load the official config to get the correct decoder architecture
    with open(args.config, 'r') as f:
        config = json.load(f)
    stage1_config = config['stage1']

    # Load decoder checkpoint once on the main process
    checkpoint = torch.load(args.decoder, map_location='cpu')
    # Pass the state dict, not the whole model, to worker processes
    decoder_state_dict = checkpoint['model_state_dict']

    npz_files = sorted(glob.glob(os.path.join(args.data_dir, '*.npz')))
    if not npz_files:
        logger.error(f"No .npz files found in {args.data_dir}")
        return

    if args.smoke:
        npz_files = npz_files[:10]

    tasks = [(path, decoder_state_dict, stage1_config, device_str) for path in npz_files]
    succ = fail = 0

    # Choose execution strategy based on arguments and device
    if args.use_threading:
        with ThreadPoolExecutor(max_workers=args.jobs) as exe:
            for ok in tqdm(exe.map(process_instance, tasks), total=len(tasks), desc="Inferring latents"):
                succ += ok
                fail += not ok
    else:
        with ProcessPoolExecutor(max_workers=args.jobs) as exe:
            for ok in tqdm(exe.map(process_instance, tasks), total=len(tasks), desc="Inferring latents"):
                succ += ok
                fail += not ok

    logger.info(f"Done → Success: {succ}, Fail: {fail}")

if __name__ == "__main__":
    main()